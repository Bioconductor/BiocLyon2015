---
title: "L5 -- Working with Large Data"
author: "Martin Morgan<br />
         Roswell Park Cancer Institute, Buffalo, NY<br />
         26-28 October, 2015"
output:
  BiocStyle::html_document:
    toc: true
    toc_depth: 2
vignette: >
  % \VignetteIndexEntry{L5 -- Working with Large Data}
  % \VignetteEngine{knitr::rmarkdown}
---

```{r style, echo = FALSE, results = 'asis'}
BiocStyle::markdown()
options(width=100, max.print=1000)
knitr::opts_chunk$set(
    eval=as.logical(Sys.getenv("KNITR_EVAL", "TRUE")),
    cache=as.logical(Sys.getenv("KNITR_CACHE", "TRUE")))
```

```{r setup, echo=FALSE, message=FALSE}
library(BiocParallel)
library(GenomicFiles)
```

- Overcoming common pitfalls in memory management and computational
  throughput when writing R code. Adopting restriction, iteration, and
  parallel evaluation strategies for working with large data. Primary
  packages: [BiocParallel][], [GenomicFiles][].

This section is adopted from material prepared by Valerie Obenchain
and Martin Morgan for the BioC 2015 annual conference.

# Summarizing overlaps

A central step in known gene RNA-seq is to summarize aligned reads in
BAM files as a count matrix. BAM files are large, but it's easy to
process them in _R_:

```{r}
library(GenomicFiles)
library(TxDb.Hsapiens.UCSC.hg19.knownGene)
txdb <- TxDb.Hsapiens.UCSC.hg19.knownGene
library(RNAseqData.HNRNPC.bam.chr14)

fls = RNAseqData.HNRNPC.bam.chr14_BAMFILES
bfls = BamFileList(fls, yieldSize=100000)
se <- summarizeOverlaps(exonsBy(txdb, "gene"), bfls)
se
se <- se[rowSums(assay(se)) != 0,]
head(assay(se))
```

Behind the scenes:

- Read files in chunks
- Evaluate files in parallel

Other solutions

- [Rsubread][] (not Windows)
- [HTSeq][] (python)

[Rsubread]: https://bioconductor.org/packages
[HTSeq]: http://www-huber.embl.de/users/anders/HTSeq/doc/overview.html

# Iteration and restriction to manage memory

When data are too large to fit in memory, we can iterate through files in
chunks or subset the data by fields or genomic positions.

Iteration

- Chunk-wise
- `open()`, read chunk(s), `close()`.
- e.g., `yieldSize` argument to `Rsamtools::BamFile()`
- Framework: `GenomicFiles::reduceByYield()`

Restriction

- Limit to columns and / or rows of interest
- Exploit domain-specific formats
    - BAM files and `Rsamtools::ScanBamParam()`
    - BAM files and `Rsamtools::PileupParam()`
    - VCF files and `VariantAnnotation::ScanVcfParam()`
- Use a data base

## Exercise: GC content

Iterate through files: `GenomicFiles::reduceByYield()`

1. Yield a chunk, restricting input to required content
2. Map from the input chunk to a possibly transformed representation
3. Reduce mapped chunks

```{r reduceByYield-setup, message=FALSE}
library(GenomicFiles)
library(GenomicAlignments)
library(Rsamtools)

yield <-     # how to input the next chunk of data
    function(X, ...)
{
    param = ScanBamParam(what="seq")
    readGAlignments(X, param=param)
}

map <-       # what to do to each chunk
    function(VALUE, ..., roi)
{
    dna = mcols(VALUE)$seq
    gc = letterFrequency(dna, "GC", as.prob=TRUE)
    tabulate(cut(gc, seq(0, 1, .05)))
}

reduce <- `+`   # how to combine chunks of GC's
```

A function to iterate through a BAM file.

```{r count-overlaps, eval=FALSE}
gc1 <- function(filename) {
    message(filename)
    ## Create and open BAM file
    bf <- BamFile(filename, yieldSize=100000)
    reduceByYield(bf, yield, map, reduce)
}
```

In action

```{r count-overlaps-doit, eval=FALSE}
gc1(fls[1])
```

# File management

## File classes

| Type  | Example use           | Name                        | Package                          |
|-------|-----------------------|-----------------------------|----------------------------------|
| .bed  | Range annotations     | `BedFile()`                 | `r Biocpkg("rtracklayer")`       |
| .wig  | Coverage              | `WigFile()`, `BigWigFile()` | `r Biocpkg("rtracklayer")`       |
| .gtf  | Transcript models     | `GTFFile()`                 | `r Biocpkg("rtracklayer")`       |
|       |                       | `makeTxDbFromGFF()`         | `r Biocpkg("GenomicFeatures")`   |
| .2bit | Genomic Sequence      | `TwoBitFile()`              | `r Biocpkg("rtracklayer")`       |
| .fastq | Reads & qualities    | `FastqFile()`               | `r Biocpkg("ShortRead")`         |
| .bam  | Aligned reads         | `BamFile()`                 | `r Biocpkg("Rsamtools")`         |
| .tbx  | Indexed tab-delimited | `TabixFile()`               | `r Biocpkg("Rsamtools")`         |
| .vcf  | Variant calls         | `VcfFile()`                 | `r Biocpkg("VariantAnnotation")` |

```{r rtracklayer-file-classes, messages=FALSE}
## rtracklayer menagerie
library(rtracklayer)
names(getClass("RTLFile")@subclasses)
```

Notes

- Not a consistent interface
- `open()`, `close()`, `import()` / `yield()` / `read*()`
- Some: selective import via index (e.g., `.bai`, BAM index);
  selection ('columns'); restriction ('rows')

## Managing a collection of files

`*FileList()` classes

- `reduceByYield()` -- iterate through a single large file
- `bplapply()` (`r Biocpkg("BiocParallel")`) -- perform independent
  operations on several files, in parallel

`GenomicFiles()`

- 'rows' as genomic range restrictions, 'columns' as files
- Each row x column is a _map_ from file data to useful representation
  in _R_
- `reduceByRange()`, `reduceByFile()`: collapse maps into summary
  representation
- see the GenomicFiles vignette [Figure 1][].

[Figure 1]: http://bioconductor.org/packages/devel/bioc/vignettes/GenomicFiles/inst/doc/GenomicFiles.pdf

# Parallel evaluation

A couple of caveats -

Iteration / restriction techniques keep the memory requirements under control
while parallel evaluation distributes the computational load across nodes. 
When running code in parallel we are still restricted by the amount of memory 
available on each node.

There is overhead in setting up and tearing down a cluster and more so when
computing in distributed memory. For small calculations, the parallel
overhead may outweigh the benefits with no improvement in performance.

Jobs that benefit the most from parallel execution are CPU-intensive
and operate on data chunks that fits into memory.

## BiocParallel

[BiocParallel][] provides a standardized interface for simple parallel
evaluation. The package builds provides access to the `snow` and
`multicore` functionality in the `parallel` package as well as
`BatchJobs` for running cluster jobs.

General ideas:

- Use `bplapply()` instead of `lapply()`
- Argument `BPPARAM` influences how parallel evaluation occurs

    - `MulticoreParam()`: threads on a single (non-Windows) machine
    - `SnowParam()`: processes on the same or different machines
    - `BatchJobsParam()`: resource scheduler on a cluster
    - `DoparParam()`: parallel execution with `foreach()` 


### Exercise: Sleeping serially and in parallel

This small example motivates the use of parallel execution and demonstrates how
`bplapply()` can be a drop in for `lapply`.

Use `system.time()` to explore how long this takes to execute as `n`
increases from 1 to 10. Use `identical()` and 
`r CRANpkg("microbenchmark")` to compare alternatives `f0()` and  `f1()`
for both correctness and performance.

`fun` sleeps for 1 second, then returns `i`.

```{r parallel-sleep}
library(BiocParallel)

fun <- function(i) {
    Sys.sleep(1)
    i
}

## serial
f0 <- function(n)
    lapply(seq_len(n), fun)

## parallel
f1 <- function(n)
    bplapply(seq_len(n), fun)
```

### Exercise: GC in parallel

Serial:

```{r, eval=FALSE}
res0 <- lapply(fls, count1)
```

Parallel:

```{r, eval=FALSE}
library(BiocParallel)
res1 <- bplapply(fls, count1)
```

## Other resources

- [GenomicFiles][] for more elaborate file management.
- [Bioconductor Amazon AMI](http://bioconductor.org/help/bioconductor-cloud-ami/)

    - easily 'spin up' 10's of instances
    - Pre-configured with Bioconductor packages and StarCluster
      management

- [GoogleGenomics][] to interact with google compute cloud and
  resources

# Efficient _R_ code

The goal of this section is to highlight practices for writing correct, robust
and efficient R code.

## Priorities

1. Correct: consistent with hand-worked examples (`identical()`, `all.equal()`)
2. Robust: supports realistic inputs, e.g., 0-length vectors, `NA`
   values, ...
3. Simple: easy to understand next month; easy to describe what it
   does to a colleague; easy to spot logical errors; easy to enhance.
4. Fast, or at least reasonable given the speed of modern computers.

## Strategies

1. Profile
    - _Look_ at the script to understand in general terms what it is doing.
    - _Step_ through the code to see how it is executed, and to gain an
      understanding of the speed of each line.
    - _Time_ evaluation of select lines or simple chunks of code with
      `system.time()` or the `r CRANpkg("microbenchmark")` package.
    - _Profile_ the code with a tool that indicates how much time is
      spent in each function call or line -- the built-in `Rprof()`
      function, or packages such as `r CRANpkg("lineprof")` or 
      `r CRANpkg("aprof")`
2. Vectorize -- operate on vectors, rather than explicit loops

    ```{r vectorize}
    x <- 1:10
    log(x)     ## NOT for (i in seq_along) x[i] <- log(x[i])
    ```

3. Pre-allocate memory, then fill in the result

    ```{r pre-allocate}
    result <- numeric(10)
    result[1] <- runif(1)
    for (i in 2:length(result))
           result[i] <- runif(1) * result[i - 1]
    result
    ```

4. Hoist common sub-expressions outside of repeated calculations, so
   that the sub-expression is only calculated once
    - Simple, e.g., 'hoist' constant multiplications from a `for` loop
    - Higher-level, e.g., use `lm.fit()` rather than repeatedly fitting
      the same design matrix.
5. Re-use existing, tested code
    - Efficient implementations of common operations -- `tabulate()`,
      `rowSums()` and friends, `%in%`, ...
    - Efficient domain-specific implementations, e.g., 
      `r Biocpkg("snpStats")` for GWAS linear models; `r Biocpkg("limma")`
      for microarray linear models; `r Biocpkg("edgeR")`, 
      `r Biocpkg("DESeq2")` for negative binomial GLMs relevant to
      RNASeq.
    - Reuse others' work -- `r Biocpkg("DESeq2")`,
      `r Biocpkg("GenomicRanges")`, `r Biocpkg("Biostrings")`, ...,
      `r CRANpkg("dplyr")`, `r CRANpkg("data.table")`, `r CRANpkg("Rcpp")`


### Case Study: Pre-allocate and vectorize 

Here's an obviously inefficient function:

```{r inefficient}
f0 <- function(n, a=2) {
    ## stopifnot(is.integer(n) && (length(n) == 1) &&
    ##           !is.na(n) && (n > 0))
    result <- numeric()
    for (i in seq_len(n))
        result[[i]] <- a * log(i)
    result
}
```

Use `system.time()` to investigate how this algorithm scales with `n`,
focusing on elapsed time.

```{r system-time}
system.time(f0(10000))
n <- 1000 * seq(1, 20, 2)
t <- sapply(n, function(i) system.time(f0(i))[[3]])
plot(t ~ n, type="b")
```

Remember the current 'correct' value, and an approximate time

```{r correct-init}
n <- 10000
system.time(expected <- f0(n))
head(expected)
```

Revise the function to hoist the common multiplier, `a`, out of the
loop. Make sure the result of the 'optimization' and the original
calculation are the same. Use the `r CRANpkg("microbenchmark")`
package to compare the two versions

```{r hoist}
f1 <- function(n, a=2) {
    result <- numeric()
    for (i in seq_len(n))
        result[[i]] <- log(i)
    a * result
}
identical(expected, f1(n))

library(microbenchmark)
microbenchmark(f0(n), f1(n), times=5)
```

Adopt a 'pre-allocate and fill' strategy

```{r preallocate-and-fill}
f2 <- function(n, a=2) {
    result <- numeric(n)
    for (i in seq_len(n))
        result[[i]] <- log(i)
    a * result
}
identical(expected, f2(n))
microbenchmark(f0(n), f2(n), times=5)
```

Use an `*apply()` function to avoid having to explicitly pre-allocate,
and make opportunities for vectorization more apparent.

```{r use-apply}
f3 <- function(n, a=2)
    a * sapply(seq_len(n), log)

identical(expected, f3(n))
microbenchmark(f0(n), f2(n), f3(n), times=10)
```

Now that the code is presented in a single line, it is apparent that
it could be easily vectorized.  Seize the opportunity to vectorize it:

```{r use-vectorize}
f4 <- function(n, a=2)
    a * log(seq_len(n))
identical(expected, f4(n))
microbenchmark(f0(n), f3(n), f4(n), times=10)
```

`f4()` definitely seems to be the winner. How does it scale with `n`?
(Repeat several times)

```{r vectorized-scale}
n <- 10 ^ (5:8)                         # 100x larger than f0
t <- sapply(n, function(i) system.time(f4(i))[[3]])
plot(t ~ n, log="xy", type="b")
```

Any explanations for the different pattern of response?

Lessons learned:

1. Vectorizing offers a huge improvement over iteration
2. Pre-allocate-and-fill is very helpful when explicit iteration is
   required.
3. `*apply()` functions help avoid need for explicit pre-allocation
   and make opportunities for vectorization more apparent. This may
   come at a small performance cost, but  is generally worth it
4. Hoisting common sub-expressions can be helpful for improving 
   performance when explicit iteration is required.

# Resources

- Lawrence, M, and Morgan, M. 2014. Scalable Genomics with R and
  Bioconductor. Statistical Science 2014, Vol. 29, No. 2,
  214-226. http://arxiv.org/abs/1409.2864v1

[BiocParallel]: https://bioconductor.org/packages/BiocParallel
[GenomicFiles]: https://bioconductor.org/packages/GenomicFiles
[GoogleGenomics]: https://bioconductor.org/packages/GoogleGenomics
